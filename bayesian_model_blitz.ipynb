{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCE_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, expression_data, meth_data):\n",
    "        super(CancerDataset, self).__init__()\n",
    "        self.dataset = list(zip(expression_data, meth_data))\n",
    "\n",
    "    def prepare_item(self, item):\n",
    "        return item[0], item[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.prepare_item(self.dataset[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "# kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  ## specify the GPU id's, GPU id's start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, mse, kl, val_loader, kl_weight):\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for expression, methylation in val_loader:\n",
    "\n",
    "            expression, methylation = expression.to(device), methylation.to(device)\n",
    "            pred = model(expression)\n",
    "\n",
    "            mse = mse_loss(pred, methylation)\n",
    "            kl = model.nn_kl_divergence()\n",
    "            cost = mse + kl_weight*kl\n",
    "\n",
    "            val_losses.append(cost.item())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.Tensor(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "sample_info = pd.read_csv(\"shared_data/cancer.samples\", sep=\"\\t\")[\n",
    "    [\"sample\", \"project_descriptor\", \"sample_type\"]\n",
    "]\n",
    "expression_df = (\n",
    "    pd.read_feather(\"shared_data/expression_df.feather\")\n",
    "    .drop([\"key\"], axis=1)\n",
    "    .transpose()\n",
    ")\n",
    "methylation_data_df = (\n",
    "    pd.read_feather(\"shared_data/methylation_data_df.feather\")\n",
    "    .drop([\"PMR_INDEX\"], axis=1)\n",
    "    .transpose()\n",
    ")\n",
    "sample_info = sample_info.set_index(\"sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df = expression_df.join(sample_info, how=\"inner\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "project_descriptor_encoded = pd.get_dummies(\n",
    "    input_data_df[\"project_descriptor\"], prefix=\"cancer_type\"\n",
    ")\n",
    "sample_type_encoded = pd.get_dummies(input_data_df[\"sample_type\"], prefix=\"sample_type\")\n",
    "data_df = pd.concat(\n",
    "    [\n",
    "        input_data_df.drop([\"project_descriptor\", \"sample_type\"], axis=1),\n",
    "        project_descriptor_encoded,\n",
    "        sample_type_encoded,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "expression_scaled = scaler.fit_transform(data_df.values.astype(np.float64))\n",
    "expression_data = torch.tensor(expression_scaled).float()\n",
    "meth_data = torch.tensor(methylation_data_df.values.astype(np.float64)).float() / 100\n",
    "expression_data = np.hstack(\n",
    "    [np.ones((expression_data.shape[0], 1)).astype(np.float64), expression_data]\n",
    ")\n",
    "expression_data = torch.tensor(expression_data).float()\n",
    "\n",
    "\n",
    "# Split Data\n",
    "expression_data_train, expression_data_test, meth_data_train, meth_data_test = (\n",
    "    train_test_split(expression_data, meth_data, test_size=0.33, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(x_data, y_data):\n",
    "    num_data = int(0.20 * len(x_data))\n",
    "    x_data_reduced = x_data[:num_data]\n",
    "    y_data_reduced = y_data[:num_data]\n",
    "    return x_data_reduced, y_data_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REDUCE_DATA:\n",
    "    x_train_reduced, y_train_reduced = reduce_data(\n",
    "        expression_data_train, meth_data_train\n",
    "    )\n",
    "    x_test_reduced, y_test_reduced = reduce_data(expression_data_test, meth_data_test)\n",
    "else:\n",
    "    x_train_reduced, y_train_reduced = expression_data_train, meth_data_train\n",
    "    x_test_reduced, y_test_reduced = expression_data_test, meth_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train_reduced.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size = y_train_reduced.shape[1]\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader preparation function\n",
    "def prepare_data_loaders(batch_size, expression_data_train, meth_data_train, expression_data_test, meth_data_test):\n",
    "    train_dataset = CancerDataset(expression_data_train, meth_data_train)\n",
    "    val_dataset = CancerDataset(expression_data_test, meth_data_test)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    valloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return trainloader, valloader\n",
    "\n",
    "\n",
    "@variational_estimator\n",
    "class BayesianModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        super(BayesianModel, self).__init__()\n",
    "        self.blinear1 = BayesianLinear(input_size, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.blinear2 = BayesianLinear(100, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if activation == \"relu\":\n",
    "            self.activation_function = nn.ReLU()\n",
    "        else:\n",
    "            self.activation_function = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blinear1(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.blinear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(\n",
    "    lr, kl_weight, batch_size, input_size, output_size, num_epochs=20, val_every=5\n",
    "):\n",
    "    trainloader, valloader = prepare_data_loaders(batch_size, x_train_reduced, y_train_reduced, x_test_reduced, y_test_reduced)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BayesianModel(input_size, output_size, \"tanh\").to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    loop = tqdm(total=len(trainloader) * num_epochs)\n",
    "    last_val_loss = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (expression, methylation) in enumerate(trainloader):\n",
    "            expression, methylation = expression.to(device), methylation.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(expression)\n",
    "            mse = mse_loss(pred, methylation)\n",
    "            kl = model.nn_kl_divergence()\n",
    "            cost = mse + kl_weight * kl\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(cost.item())\n",
    "            if (i + 1) % val_every == 0:\n",
    "                val_loss = evaluate(model, mse, kl, valloader, kl_weight)\n",
    "                last_val_loss = val_loss\n",
    "                val_losses.append((len(train_losses), val_loss))\n",
    "            loop.set_description(f'Epoch: {epoch + 1}, ' + 'Train - Cost: %2.2f MSE : %2.2f, KL : %2.2f, ' % (cost.item(), mse.item(), kl.item()) + 'Val loss: {:.4f}'.format(last_val_loss.item() ) if last_val_loss is not None else '')\n",
    "            loop.update(1)\n",
    "\n",
    "    return train_losses, val_losses, last_val_loss, model, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, last_val_loss):\n",
    "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
    "    plt.title(\"Linear Model Gene Expression to Methylaton\")\n",
    "\n",
    "    plt.xlabel('Train Time')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    x, val_loss = zip(*val_losses)\n",
    "    plt.plot(x, val_loss, label='Val Loss')\n",
    "    plt.title(\"Linear Model Gene Expression to Methylaton\")\n",
    "    x_text = (len(x)) * 0.05\n",
    "    y_text = max(val_loss)\n",
    "    plt.text(\n",
    "        x_text,\n",
    "        y_text,\n",
    "        f\"MSE: {last_val_loss.item():.2f}\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    plt.xlabel('Train Time')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_vs_actual(model, valloader):\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for expression, methylation in valloader:\n",
    "\n",
    "            expression, methylation = expression.to(device), methylation.to(device)\n",
    "            pred = model(expression)\n",
    "            actual.extend(methylation.cpu().numpy().flatten().tolist())\n",
    "            predicted.extend(pred.cpu().numpy().flatten().tolist())\n",
    "\n",
    "\n",
    "    pearson_corr, _ = pearsonr(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    # Plot the Results\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.xlabel(\"True Methylation Levels\")\n",
    "    plt.ylabel(\"Predicted Methylation Levels\")\n",
    "    plt.title(\"Neual Network Gene Expression Regressor Predictions vs True Values\")\n",
    "    plt.plot(\n",
    "        [min(actual), max(predicted)],\n",
    "        [min(actual), max(predicted)],\n",
    "        \"k--\",\n",
    "        lw=4,\n",
    "    )\n",
    "\n",
    "    x_text = min(actual) + (max(actual) - min(actual)) * 0.9\n",
    "    y_text = min(predicted) + (max(predicted) - min(predicted)) * 0.05\n",
    "\n",
    "    plt.text(\n",
    "        x_text,\n",
    "        y_text,\n",
    "        f\"MSE: {mse:.2f}\\nRÂ²: {r2:.2f}\\nPearson Correlation: {pearson_corr:.2f}\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rates = [1e-3, 0.5e-3, 1e-4, 1e-5]\n",
    "kl_weights = [0.01, 0.1, 0.5, 1]\n",
    "batch_sizes = [10, 50, 100]\n",
    "epochs = [20, 100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.0005, 0.0001, 1e-05]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = [5]\n",
    "# batch_sizes = [10]\n",
    "# kl_weights = [0.01]\n",
    "# learning_rates = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, kl_weight=0.01, batch_size=10, epochs=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490c4bffdc546be9a418b66d2839fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate over all combinations\n",
    "for epoch in epochs:\n",
    "    for lr in learning_rates:\n",
    "        for kl_weight in kl_weights:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\n",
    "                    f\"Training with lr={lr}, kl_weight={kl_weight}, batch_size={batch_size}, epochs={epoch}\"\n",
    "                )\n",
    "                train_losses, val_losses, last_val_loss, model, valloader = train_model(\n",
    "                    lr, kl_weight, batch_size, input_size, output_size, num_epochs=epoch\n",
    "                )\n",
    "                plot_losses(train_losses, val_losses, last_val_loss)\n",
    "                pred_vs_actual(model, valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
